{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "IMDB text classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0n9R-_Z9rMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82e227e-8d7a-40d6-94ce-e203b829db5c"
      },
      "source": [
        "#hide\n",
        "#skip\n",
        "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab\n",
        "from fastai.text.all import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 11.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilGqNe_x9rMc"
      },
      "source": [
        "# Transfer learning in text\n",
        "\n",
        "> How to fine-tune a language model and train a classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUF26gVZ9rMd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b8905ee4-542d-427c-d1e2-3964dee4ee7c"
      },
      "source": [
        "data = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaUd9NpE9rMe"
      },
      "source": [
        "We can then have a look at the data with the `show_batch` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZyDomp9rMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "cab7cce0-2a30-48d4-c5e4-41b8a9b269b9"
      },
      "source": [
        "data.show_batch()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos xxmaj some have praised _ xxunk _ as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n\\n xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of many older adventure movies has been done well before , ( think _ the xxmaj dirty xxmaj dozen _ ) but _ atlantis _ represents one of the worse films in that motif . xxmaj the characters are weak . xxmaj even the background that each member trots out seems stock and awkward at best . xxmaj an xxup md / xxmaj medicine xxmaj man , a tomboy mechanic whose father always wanted sons , if we have not at least seen these before</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of oatmeal . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain overconfidence on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an idyllic storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xxbos xxmaj heavy - handed moralism . xxmaj writers using characters as mouthpieces to speak for themselves . xxmaj predictable , plodding plot points ( say that five times fast ) . a child 's imitation of xxmaj britney xxmaj spears . xxmaj this film has all the earmarks of a xxmaj lifetime xxmaj special reject . \\n\\n i honestly believe that xxmaj jesus xxmaj xxunk and xxmaj julia xxmaj xxunk set out to create a thought - provoking , emotional film on a tough subject , exploring the idea that things are not always black and white , that one who is a criminal by definition is not necessarily a bad human being , and that there can be extenuating circumstances , especially when one puts the well - being of a child first . xxmaj however , their earnestness ends up being channeled into preachy dialogue and trite</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>xxbos xxmaj here are the matches . . . ( adv . = advantage ) \\n\\n xxmaj the xxmaj warriors ( ultimate xxmaj warrior , xxmaj texas xxmaj tornado and xxmaj legion of xxmaj doom ) v xxmaj the xxmaj perfect xxmaj team ( mr xxmaj perfect , xxmaj ax , xxmaj smash and xxmaj crush of xxmaj demolition ) : xxmaj ax is the first to go in seconds when xxmaj warrior splashes him for the pin ( 4 - 3 adv . xxmaj warriors ) . i knew xxmaj ax was n't a healthy man but if he was that unhealthy why bother have him on the card ? xxmaj this would be his last xxup ppv . xxmaj eventually , both xxmaj legion of xxmaj doom and xxmaj demolition job out cheaply via double disqualification ( 2 - 1 adv . xxmaj warriors ) . xxmaj perfect</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>xxbos xxup spoilers xxup herein \\n\\n xxmaj my xxmaj high xxmaj school did all they could to try and motivate us for exams . xxmaj but the most memorable method they used to get us into the right state of mind was a guest speaker , who was none other than xxmaj australian xxmaj kickboxing 's favorite son , xxmaj stan \" the xxmaj man \" xxmaj xxunk . xxmaj the first mistake they made was giving this guy a microphone , because he was screaming half the time despite us sitting no more than 3 or 4 feet away from him . xxmaj now , his speech was full of the usual \" if you fail to prepare , then prepare to fail \" stuff , but there were various instances where i got really worked up . xxmaj the guy stood there in front of us preaching how</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxbos xxmaj who knew ? xxmaj dowdy xxmaj queen xxmaj victoria , the plump xxmaj monarch who was a virtual recluse for 40 years after the death of her husband , xxmaj prince xxmaj albert , actually led a life fraught with drama and intrigue in her younger days . ' the xxmaj young xxmaj victoria ' not only chronicles the young xxmaj queen 's romance with her husband - to - be but also does a pretty good job of detailing the political machinations surrounding her ascent to the throne . \\n\\n xxmaj the xxmaj act i ' set - up ' draws you in right away . xxmaj following the death of xxmaj victoria 's father , the xxmaj duke of xxmaj kent in 1820 , less than a year after xxmaj victoria 's birth , the xxmaj duchess of xxmaj kent eventually hooked up with former xxmaj</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>xxbos xxmaj well , i finally saw it . i did n't go when it first came out because , well , frankly , i was afraid . xxmaj afraid of how bad it might be , or how disappointing . xxmaj while not as bad as xxmaj menace , and better than xxmaj clones , it was n't particularly memorable , or satisfying . \\n\\n i was 11 years old when i saw xxmaj star xxmaj wars . i still remember sitting in the theater . xxmaj from the opening crawl to the final credits it was a movie experience xxmaj i 'll never forget . a timeless story of the bored farm - boy who just knows he was meant for more , saving the princess and the xxmaj galaxy from the evil menace while being mentored by the wise wizard , the rogue pirate and the various</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>xxbos xxmaj after reading the previous comments , xxmaj i 'm just glad that i was n't the only person left confused , especially by the last 20 minutes . xxmaj john xxmaj carradine is shown twice walking down into a grave and pulling the lid shut after him . i anxiously awaited some kind of explanation for this odd behavior … naturally i assumed he had something to do with the evil goings - on at the house , but since he got killed off by the first rising corpse ( hereafter referred to as xxmaj zombie # 1 ) , these scenes made absolutely no sense . xxmaj please , if someone out there knows why xxmaj carradine kept climbing down into graves -- let the rest of us in on it ! ! \\n\\n xxmaj all the action is confined to the last 20 minutes so xxmaj</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHHpRRYR9rMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9e4df9de-ce80-4fe1-c313-54a353c2a13e"
      },
      "source": [
        "learn = text_classifier_learner(data, AWD_LSTM, metrics=accuracy)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGbLKjY69rMe"
      },
      "source": [
        "We use the [AWD LSTM](https://arxiv.org/abs/1708.02182) architecture, `drop_mult` is a parameter that controls the magnitude of all dropouts in that model, and we use `accuracy` to track down how well we are doing. We can then fine-tune our pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFB-ym649rMe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "68496b45-ce6f-42bc-f3d4-1057e5e17999"
      },
      "source": [
        "learn.fine_tune(0, 1e-2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.606366</td>\n",
              "      <td>0.398067</td>\n",
              "      <td>0.821400</td>\n",
              "      <td>07:31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quVCLPz39rMf",
        "outputId": "5ab8ec78-b7bb-4e3f-f4cb-1cd6f1f3b9ca"
      },
      "source": [
        "learn.fine_tune(4, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.594912</td>\n",
              "      <td>0.407416</td>\n",
              "      <td>0.823640</td>\n",
              "      <td>01:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.268259</td>\n",
              "      <td>0.316242</td>\n",
              "      <td>0.876000</td>\n",
              "      <td>03:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.184861</td>\n",
              "      <td>0.246242</td>\n",
              "      <td>0.898080</td>\n",
              "      <td>03:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.136392</td>\n",
              "      <td>0.220086</td>\n",
              "      <td>0.918200</td>\n",
              "      <td>03:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.106423</td>\n",
              "      <td>0.191092</td>\n",
              "      <td>0.931360</td>\n",
              "      <td>03:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afwzlV6G9rMf"
      },
      "source": [
        "Not too bad! To see how well our model is doing, we can use the `show_results` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoQxZhwT9rMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "outputId": "c8153ba5-1a50-4b6f-b572-95436f5668b4"
      },
      "source": [
        "learn.show_results()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>category_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj there 's a sign on xxmaj the xxmaj lost xxmaj highway that says : \\n\\n * major xxup spoilers xxup ahead * \\n\\n ( but you already knew that , did n't you ? ) \\n\\n xxmaj since there 's a great deal of people that apparently did not get the point of this movie , xxmaj i 'd like to contribute my interpretation of why the plot makes perfect sense . xxmaj as others have pointed out , one single viewing of this movie is not sufficient . xxmaj if you have the xxup dvd of xxup md , you can \" cheat \" by looking at xxmaj david xxmaj lynch 's \" top 10 xxmaj hints to xxmaj unlocking xxup md \" ( but only upon second or third viewing , please . ) ;) \\n\\n xxmaj first of all , xxmaj mulholland xxmaj drive is</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>xxbos i remember vacationing in xxmaj florida when this movie aired . i had set up my xxup vcr to record it . xxmaj the anticipation was killing me . i had known about the movie ever since it was announced some half a year earlier . xxmaj we came back from xxmaj florida 4 days after the movie aired , and i immediately watched it . i tried as hard as i could to like it , but i did n't . \\n\\n i am a xxup huge 3 stooges fan . xxmaj and as such i know quite a bit about them . xxmaj so it was n't like i was expecting to learn anything from the movie , and i did n't . i was more interested with the portrayals and seeing how accurate their information was . xxmaj there were many things wrong with this film</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xxbos xxmaj this is one of four 1970s movies by xxup tv writer xxmaj lane xxmaj slate about sensationalistic murders in small towns . xxmaj they feature likable xxup tv personalities as police chiefs and quirky characters as town regulars , including light - touch love interests . xxmaj the others are : xxmaj they xxmaj only xxmaj kill xxmaj their xxmaj masters ( james xxmaj garner , 1972 ) ; and xxmaj the xxmaj girl xxmaj in xxmaj the xxmaj empty xxmaj grave and xxmaj deadly xxmaj game ( both xxmaj andy xxmaj griffith , 1977 ) . \\n\\n xxmaj alda 's is set near xxmaj vermont ( \" mount xxmaj angel \" next to \" horse xxmaj creek \" ) ; the others in xxmaj california , xxmaj garner 's at seaside ( \" eden xxmaj landing \" ) and xxmaj griffith 's in the mountains ( \"</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>xxbos xxmaj hearkening back to those \" good xxmaj old xxmaj days \" of 1971 , we can vividly recall when we were treated with a whole xxmaj season of xxmaj charles xxmaj chaplin at the xxmaj cinema . xxmaj that 's what the promotional guy called it when we saw him on somebody 's old talk show . ( we ca n't recall just whose it was ; either xxup merv xxup griffin or xxup woody xxup woodbury , one or the other ! ) xxmaj the guest talked about xxmaj sir xxmaj charles ' career and how his films had been out of circulation ever since the 1952 exclusion of the former \" little xxmaj tramp ' from xxmaj los xxmaj xxunk xxmaj xxunk on the grounds of his being an \" undesirable xxmaj alien \" . ( no xxmaj schultz , he 's xxup not from another</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>xxbos xxmaj the film had xxup no help at all , promotion - wise : if there was an advertising promo on xxup tv or radio , i did n't see / hear it . xxmaj the only newspaper ad i saw was on it 's opening weekend : a dingy , xxunk b &amp; w head - shot photo of xxmaj andy as val - com , behind jail bars , with headline : \" wanted ! xxmaj runaway xxmaj robot ! \" ( which was also the poster in front of the 3 movie theaters i saw it at xxunk the nice little color poster on this site , with headshots of all the cast , and cartoon of xxmaj xxunk xxunk really was n't xxup that good -- they xxup ought to have used an action scene from the film itself -- didn't they have an onset</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>xxbos xxmaj this enjoyable xxmaj euro - western opens with a scene that predates a similar scene that xxmaj sergio xxmaj leone wanted to shoot for \" once xxmaj upon a xxmaj time in the xxmaj west \" but could n't persuade xxmaj clint xxmaj eastwood to appear in . xxmaj three tough - looking gunfighters ride into a town . xxmaj one is dressed like the xxmaj man with xxmaj no xxmaj name in a poncho . xxmaj another is dressed like xxmaj colonel xxmaj mortimer from \" for a xxmaj few xxmaj dollars xxmaj more , \" and the third is garbed like xxmaj django , except he rides a horse instead of pulls a coffin behind him with a machine gun in it . xxmaj our hero meets them in xxmaj main xxmaj street behind a wagon loaded with three coffins . \" any xxmaj gun xxmaj</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>xxbos i do not think i am alone when i say that 2005 has not been particularly kind to the horror genre . xxmaj while \" cursed \" , \" hide and xxmaj seek \" , \" the xxmaj ring xxmaj two \" , and \" the xxmaj amityville xxmaj horror \" all showed glimpses of interest and potential , there have been more misses than hits . xxmaj for proof , see : \" white xxmaj noise \" , \" boogeyman \" , \" the xxmaj jacket \" , \" mindhunters \" , and \" alone in the xxmaj dark \" . xxmaj imagine my surprise when \" house of xxmaj wax \" , tightly written by siblings xxmaj chad and xxmaj carey xxmaj hayes , turned out to be … well , a surprise . \\n\\n xxmaj carly xxmaj jones ( elisha xxmaj cuthbert ) is a young</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>xxbos xxmaj danny xxmaj boyle was not the first person to realise that zombies can run like the xxunk . xxmaj that honour belongs to xxmaj lifeforce , which is , of course , the greatest naked space vampire zombies from xxmaj halley 's xxmaj comet running amok in xxmaj london end - of - the - world movie ever made . xxmaj tobe xxmaj hooper may have made a lot of crap , but for this deliriously demented epic sci - fi horror he deserves a place among the immortals . xxmaj plus it offers space vampire xxmaj mathilda xxmaj may , the best thing to come out of xxmaj france since xxmaj simone xxmaj simon , spending the entire movie naked . xxmaj which she does very , very well . xxmaj just bear in mind that while she is the most overwhelmingly feminine presence anyone on xxmaj</td>\n",
              "      <td>pos</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>xxbos xxmaj why would xxmaj burt xxmaj lancaster allow himself to play a poor schnook who is ultimately undermined by femme fatale xxmaj anna xxmaj dundee , played by xxmaj yvonne decarlo in ' criss xxmaj cross ' ? xxmaj the same reason why xxmaj robert xxmaj mitchum allows himself to be cast as another loser who falls for femme fatale xxmaj faith xxmaj domergue in the 1950 noir , \" where xxmaj danger xxmaj lives \" . xxmaj perhaps they both felt it was a good way to show that they had ' range ' as xxunk playing against type , the usual ' tough - guy ' role they were known for , would enhance their image as actors who could play any role . xxmaj but the problem was that roles like xxmaj steve xxmaj thompson , the pathetic love - sick milquetoast in ' criss xxmaj</td>\n",
              "      <td>neg</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdPgyg0S9rMf"
      },
      "source": [
        "And we can predict on new texts quite easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BjIHJms9rMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e19a992b-5371-419f-8212-09bb524da15e"
      },
      "source": [
        "learn.predict(\"I really liked that movie!\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([0.1256, 0.8744]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ti1qR__kCbPk",
        "outputId": "ded274cf-2f38-45af-c07c-8bcd1b812c48"
      },
      "source": [
        "learn.predict(\"I really hated that movie!\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('neg', tensor(0), tensor([0.8156, 0.1844]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YcgVVou4D48o",
        "outputId": "9ea7fcd1-6478-46b6-a520-ee416633bc20"
      },
      "source": [
        "learn.predict(\"I am opposed to his teaching philosophy\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([0.4616, 0.5384]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSN0HZqW9rMg"
      },
      "source": [
        "Here we can see the model has considered the review to be positive. The second part of the result is the index of \"pos\" in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for \"pos\" and 0.9% for \"neg\"). \n",
        "\n",
        "Now it's your turn! Write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZtIlAkt9rMg"
      },
      "source": [
        "### Using the data block API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO0nGwB99rMg"
      },
      "source": [
        "We can also use the data block API to get our data in a `DataLoaders`. This is a bit more advanced, so fell free to skip this part if you are not comfortable with learning new APIs just yet.\n",
        "\n",
        "A datablock is built by giving the fastai library a bunch of information:\n",
        "\n",
        "- the types used, through an argument called `blocks`: here we have images and categories, so we pass `TextBlock` and `CategoryBlock`. To inform the library our texts are files in a folder, we use the `from_folder` class method.\n",
        "- how to get the raw items, here our function `get_text_files`.\n",
        "- how to label those items, here with the parent folder.\n",
        "- how to split those items, here with the grandparent folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Talky5lg9rMg"
      },
      "source": [
        "imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock),\n",
        "                 get_items=get_text_files,\n",
        "                 get_y=parent_label,\n",
        "                 splitter=GrandparentSplitter(valid_name='test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xb-wnfB9rMg"
      },
      "source": [
        "This only gives a blueprint on how to assemble the data. To actually create it, we need to use the `dataloaders` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7f_qroX9rMg"
      },
      "source": [
        "dls = imdb.dataloaders(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2esn2TF9rMg"
      },
      "source": [
        "## The ULMFiT approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG4KuMEi9rMg"
      },
      "source": [
        "The pretrained model we used in the previous section is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and *then* use that as the base for our classifier.\n",
        "\n",
        "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles.\n",
        "\n",
        "The whole process is summarized by this picture:\n",
        "\n",
        "![ULMFit process](https://github.com/fastai/fastai/blob/master/nbs/images/ulmfit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgK6ceq9rMg"
      },
      "source": [
        "### Fine-tuning a language model on IMDb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnip38vi9rMg"
      },
      "source": [
        "We can get our texts in a `DataLoaders` suitable for language modeling very easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3qY3NWm9rMg"
      },
      "source": [
        "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJevGsjG9rMg"
      },
      "source": [
        "We need to pass something for `valid_pct` otherwise this method will try to split the data by using the grandparent folder names. By passing `valid_pct=0.1`, we tell it to get a random 10% of those reviews for the validation set.\n",
        "\n",
        "We can have a look at our data using `show_batch`. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r48PTPqI9rMg",
        "outputId": "7692fdd9-3366-4c78-b813-8a58d30b68cf"
      },
      "source": [
        "dls_lm.show_batch(max_n=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj about thirty minutes into the film , i thought this was one of the weakest \" xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into</td>\n",
              "      <td>xxmaj about thirty minutes into the film , i thought this was one of the weakest \" xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>yeon . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see</td>\n",
              "      <td>. xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see '</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tends to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj</td>\n",
              "      <td>to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>movie just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on</td>\n",
              "      <td>just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on par</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>greetings again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj</td>\n",
              "      <td>again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj the</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1RDOLDS9rMg"
      },
      "source": [
        "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. `to_fp16` puts the `Learner` in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_vouAGl9rMg"
      },
      "source": [
        "learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3_5fb8A9rMg"
      },
      "source": [
        "By default, a pretrained `Learner` is in a frozen state, meaning that only the head of the model will train while the body stays frozen. We show you what is behind the fine_tune method here and use a fit_one_cycle method to fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dl7STb539rMg",
        "outputId": "56909ed3-964c-47f4-ba36-e54fb9606cad"
      },
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.120048</td>\n",
              "      <td>3.912788</td>\n",
              "      <td>0.299565</td>\n",
              "      <td>50.038246</td>\n",
              "      <td>11:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_z1bnd69rMg"
      },
      "source": [
        "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnJBTBAk9rMg"
      },
      "source": [
        "You can easily save the state of your model like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_95aqa89rMg"
      },
      "source": [
        "learn.save('1epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt3U-aC_9rMg"
      },
      "source": [
        "It will create a file in `learn.path/models/` named \"1epoch.pth\". If you want to load your model on another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRNfj0VG9rMg"
      },
      "source": [
        "learn = learn.load('1epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqc4e5ct9rMg"
      },
      "source": [
        "We can them fine-tune the model after unfreezing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZtc9sPx9rMg",
        "outputId": "2130583f-ce4c-4588-e95a-e49d8bdeb8bb"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.893486</td>\n",
              "      <td>3.772820</td>\n",
              "      <td>0.317104</td>\n",
              "      <td>43.502548</td>\n",
              "      <td>12:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.820479</td>\n",
              "      <td>3.717197</td>\n",
              "      <td>0.323790</td>\n",
              "      <td>41.148880</td>\n",
              "      <td>12:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.735622</td>\n",
              "      <td>3.659760</td>\n",
              "      <td>0.330321</td>\n",
              "      <td>38.851997</td>\n",
              "      <td>12:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.677086</td>\n",
              "      <td>3.624794</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>37.516987</td>\n",
              "      <td>12:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.636646</td>\n",
              "      <td>3.601300</td>\n",
              "      <td>0.337017</td>\n",
              "      <td>36.645859</td>\n",
              "      <td>12:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.553636</td>\n",
              "      <td>3.584241</td>\n",
              "      <td>0.339355</td>\n",
              "      <td>36.026001</td>\n",
              "      <td>12:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>3.571892</td>\n",
              "      <td>0.341353</td>\n",
              "      <td>35.583862</td>\n",
              "      <td>12:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.444101</td>\n",
              "      <td>3.565988</td>\n",
              "      <td>0.342194</td>\n",
              "      <td>35.374371</td>\n",
              "      <td>12:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.398597</td>\n",
              "      <td>3.566283</td>\n",
              "      <td>0.342647</td>\n",
              "      <td>35.384815</td>\n",
              "      <td>12:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.375563</td>\n",
              "      <td>3.568166</td>\n",
              "      <td>0.342528</td>\n",
              "      <td>35.451500</td>\n",
              "      <td>12:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TCYvmPk9rMi"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqS178Hq9rMi"
      },
      "source": [
        "learn.save_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG_P2hcO9rMi"
      },
      "source": [
        "> Jargon: Encoder: The model not including the task-specific final layer(s). It means much the same thing as *body* when applied to vision CNNs, but tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GliFbLta9rMi"
      },
      "source": [
        "Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR5rq4BT9rMi",
        "outputId": "f823a50e-00f2-4862-c36b-4e3ac0953e59"
      },
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHpeVjMu9rMj",
        "outputId": "2176300c-4786-47d8-f143-1da7bd3e41ed"
      },
      "source": [
        "print(\"\\n\".join(preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story\n",
            "i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the \" evil \" machine has to be used to protect\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5QpOcXK9rMj"
      },
      "source": [
        "### Training a text classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApbjlYFr9rMj"
      },
      "source": [
        "We can gather our data for text classification almost exactly like before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4-ECBD99rMj"
      },
      "source": [
        "dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', text_vocab=dls_lm.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUxe4U2Q9rMj"
      },
      "source": [
        "The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with `text_vocab`.\n",
        "\n",
        "Then we can define our text classifier like before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLCAS-sq9rMj"
      },
      "source": [
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Djk3ib9rMj"
      },
      "source": [
        "The difference is that before training it, we load the previous encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEsm1i9y9rMj"
      },
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf7Gx9ZZ9rMj"
      },
      "source": [
        "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyrFvCML9rMj",
        "outputId": "c8fbed94-16a7-48b4-a4cd-6bb450345989"
      },
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.347427</td>\n",
              "      <td>0.184480</td>\n",
              "      <td>0.929320</td>\n",
              "      <td>00:33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Riw_i509rMj"
      },
      "source": [
        "In just one epoch we get the same result as our training in the first section, not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVZqB8gn9rMj",
        "outputId": "b3f1d39e-92ba-4ac6-9099-8975cf47e4e0"
      },
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.247763</td>\n",
              "      <td>0.171683</td>\n",
              "      <td>0.934640</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2VmMXQr9rMj"
      },
      "source": [
        "Then we can unfreeze a bit more, and continue training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjgP-VgF9rMj",
        "outputId": "7c92a418-18a1-4ac1-b492-4239fbb91482"
      },
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.193377</td>\n",
              "      <td>0.156696</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>00:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVRfI3789rMj"
      },
      "source": [
        "And finally, the whole model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN6_s6P29rMj",
        "outputId": "5550fc83-1f2c-4d7d-8e08-f907efa9cf93"
      },
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.172888</td>\n",
              "      <td>0.153770</td>\n",
              "      <td>0.943120</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.161492</td>\n",
              "      <td>0.155567</td>\n",
              "      <td>0.942640</td>\n",
              "      <td>00:57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGAUXlkH9rMk"
      },
      "source": [
        ""
      ]
    }
  ]
}